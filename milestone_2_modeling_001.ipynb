{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression, ElasticNet, Ridge, Lasso\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"data\\\\milestone_data_X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"data\\\\milestone_data_y_train.pkl\")\n",
    "X_test = pd.read_pickle(\"data\\\\milestone_data_X_test.pkl\")\n",
    "y_test = pd.read_pickle(\"data\\\\milestone_data_y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_pickle(\"data\\\\milestone_data_X_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (221122, 1095)\n",
      "y_train shape (221122,)\n",
      "X_test shape (32257, 1095)\n",
      "y_test shape (32257,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Check there are no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_NaN = X_train.isnull()\n",
    "col_has_NaN = is_NaN.any(axis=0)\n",
    "col_has_NaN = col_has_NaN.loc[col_has_NaN==True].index.to_list()\n",
    "col_has_NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train._get_numeric_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that contain infinity or negative infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume_adi', 'trend_vortex_ind_pos', 'trend_vortex_ind_neg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_has_inf = X_train_numeric.columns.to_series()[np.isinf(X_train_numeric).any()].to_list()\n",
    "col_has_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train_numeric.drop(col_has_inf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train_numeric.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianRidge(compute_score=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = BayesianRidge(compute_score=True)\n",
    "model.fit(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred_train = model.predict(X_train[cols])\n",
    "y_pred_test = model.predict(X_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that this will run on the validation data - we won't score against the validation data until once at the very end\n",
    "y_pred_val = model.predict(X_val[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring\n",
    "First using a typical regression metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the training score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05871998787118553"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06651413809583322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score is slightly worse than the train score but the difference does not seem huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now converting to a classification metric. We will test for precision. We will first establish a benchmark by calculating what our precision would be if we simply predict that everything goes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_clf_pred(pred, threshold=1):\n",
    "    pred_clf = (pred > threshold) * 1 # multiply by 1 to change to binary from True / False\n",
    "    \n",
    "    return pred_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clf = pred_to_clf_pred(y_train, threshold=1)\n",
    "y_test_clf = pred_to_clf_pred(y_test, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_up_train = [1] * len(y_train_clf)\n",
    "y_all_up_test = [1] * len(y_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_precision_train_score = precision_score(y_train_clf, y_all_up_train)\n",
    "benchmark_precision_test_score = precision_score(y_test_clf, y_all_up_test)\n",
    "benchmark_return_train = np.mean(y_train) - 1\n",
    "benchmark_return_test = np.mean(y_test) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_benchmarks():\n",
    "    print(\"Benchmark figures over 20 day prediction horizon:\")\n",
    "    print(\"benchmark precision train:\", round(benchmark_precision_train_score,5))\n",
    "    print(\"benchmark precision test:\", round(benchmark_precision_test_score,5))\n",
    "    print(\"benchmark return train:\", round(benchmark_return_train,5))\n",
    "    print(\"benchmark return test:\", round(benchmark_return_test,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark figures over 20 day prediction horizon:\n",
      "benchmark precision train: 0.53814\n",
      "benchmark precision test: 0.5835\n",
      "benchmark return train: 0.005\n",
      "benchmark return test: 0.02214\n"
     ]
    }
   ],
   "source": [
    "print_benchmarks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the benchmark precision if we predict everything goes up. Now we will test the precision of our model. We can see that market conditions in the test period were noticeably different to the training period as stocks were going up a lot more frequently over 20 day trading horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_clf_train = pred_to_clf_pred(y_pred_train, threshold=1)\n",
    "#y_pred_clf = pred_to_clf_pred(y_pred_test, threshold=1) #(y_pred_test > 1) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the precision score on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_precision(y, pred, percentile):\n",
    "    threshold = np.percentile(pred, percentile)\n",
    "    pred_clf = (pred > threshold) * 1\n",
    "    threshold_precision = precision_score(y, pred_clf)\n",
    "    \n",
    "    return round(threshold_precision,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_and_threshold_scoring(y, y_pred, percentile):\n",
    "    threshold = np.percentile(y_pred, percentile)\n",
    "    results = {}\n",
    "    y_clf = pred_to_clf_pred(y, threshold=1)\n",
    "    y_pred_clf = pred_to_clf_pred(y_pred, threshold=1)\n",
    "    results['default_precision'] = round(precision_score(y_clf, y_pred_clf),5)\n",
    "    results['default_return'] = round(np.mean(y[y_pred>1])-1,5)\n",
    "    results['threshold_precision'] = threshold_precision(y_clf, y_pred, percentile)\n",
    "    results['threshold_return'] = round(np.mean(y[y_pred>threshold])-1,5)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate model peformance on train and test using the metrics specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark figures over 20 day prediction horizon:\n",
      "benchmark precision train: 0.53814\n",
      "benchmark precision test: 0.5835\n",
      "benchmark return train: 0.005\n",
      "benchmark return test: 0.02214\n",
      "train: {'default_precision': 0.57186, 'default_return': 0.01559, 'threshold_precision': 0.70779, 'threshold_return': 0.08725}\n",
      "test:  {'default_precision': 0.57052, 'default_return': 0.01887, 'threshold_precision': 0.52821, 'threshold_return': 0.01978}\n"
     ]
    }
   ],
   "source": [
    "print_benchmarks()\n",
    "print(\"train:\",full_and_threshold_scoring(y_train,y_pred_train,95))\n",
    "print(\"test: \",full_and_threshold_scoring(y_test,y_pred_test,95))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results\n",
    "Here we can see that in our training data we achieved an improvement over the benchmark for precision and return where we set the precision threshold to predict any stock with a prediction of the price going up to go up. When we made our threshold more restrictive, so that we only predicted the stock would go up for our 5% highest predictions, on our train data the precision and return vastly outperformed the benchmark.\n",
    "#### Test Results\n",
    "On the test data we did not beat the benchmark when we classified all predictions that a stock would go up as the stock would go up. When we only indicated the stock would go up for our highest 5% of predictions, our precision is even worse although the average return is slightly better than at the default threshold. Now we will need to use some supervised learning techniques to select features and try with different models to improve on this precision score and aim get it above the benchmark. The issue is likely to be partly overfitting and partly model drift as we saw from the benchmark precision the market conditions were very different in the test period to the training period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Fundamental Data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_cols = ['SR', 'QualityRank', 'ValueRank', 'MomentumRank', 'Beta', 'Free_Float_pct', 'Price_to_Sales_TTM', \n",
    "'Price_to_FCF_TTM', 'Price_to_Book_Latest', 'P_TB_Latest', 'Yield_TTM_pct', 'EV_to_EBITDA_TTM', 'Magic_Formula_Score_Percentile', 'EPS_Growth_TTM_pct', 'Sales_Growth_TTM_pct', \n",
    "'Exp_Return_Sustainable_Growth_pct', 'ROCE_TTM_pct', 'ROE_TTM_pct', 'Piotroski_F_Score_TTM', 'Price_vs_MA_50d_pct', 'Price_vs_MA_200d_pct'\n",
    "] #ignore Sector for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_cols = ['SR', 'ROCE_TTM_pct', 'Price_to_FCF_TTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=3, n_jobs=-1, random_state=6)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# model = BayesianRidge(compute_score=True) # doesn't beat test benchmark precision\n",
    "# model = LinearRegression() # doesn't beat test benchmark precision\n",
    "# model = ElasticNet(random_state = 1357) # zero precision on test\n",
    "# model = Ridge() # doesn't beat test benchmark precision\n",
    "# model = Lasso() # zero precision on test\n",
    "# model = tree.DecisionTreeRegressor() # beats benchmark using 'SR' feature\n",
    "# random forest criterion \"mae\" is much slower than \"mse\"\n",
    "model = RandomForestRegressor(max_depth=3, random_state=6, criterion=\"mse\", n_jobs=-1) #, min_impurity_decrease=0.01) # \n",
    "#model = GradientBoostingRegressor(subsample=0.95, max_depth=3, verbose=0, random_state=42, n_estimators=200)\n",
    "# model = KNeighborsRegressor()\n",
    "\n",
    "model.fit(X_train_numeric[fundamental_cols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 432 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make the predictions\n",
    "y_pred_fundamental_train = model.predict(X_train[fundamental_cols])\n",
    "y_pred_fundamental_test = model.predict(X_test[fundamental_cols])\n",
    "# convert to classification\n",
    "y_pred_clf_fundamental_train = pred_to_clf_pred(y_pred_fundamental_train, threshold=1)\n",
    "y_pred_clf_fundamental_test = pred_to_clf_pred(y_pred_fundamental_test, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model has performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark figures over 20 day prediction horizon:\n",
      "benchmark precision train: 0.53814\n",
      "benchmark precision test: 0.5835\n",
      "benchmark return train: 0.005\n",
      "benchmark return test: 0.02214\n",
      "train: {'default_precision': 0.53855, 'default_return': 0.00513, 'threshold_precision': 0.57369, 'threshold_return': 0.02397}\n",
      "test:  {'default_precision': 0.58331, 'default_return': 0.02219, 'threshold_precision': 0.63341, 'threshold_return': 0.05528}\n"
     ]
    }
   ],
   "source": [
    "print_benchmarks()\n",
    "print(\"train:\",full_and_threshold_scoring(y_train,y_pred_fundamental_train,95))\n",
    "print(\"test: \",full_and_threshold_scoring(y_test,y_pred_fundamental_test,95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just a single feature \"SR\" which is the proprietary feature from Stockopedia known as the StockRank, we have beaten the benchmark precision on train and test both at the default and when we restrict our predictions that the stock will go up to the top 5% of all our regression predictions. The margin over the benchmark for both train and test is bigger for the top 5% of predictions compared to all predictions where the regression model predicted any return greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Feature Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
